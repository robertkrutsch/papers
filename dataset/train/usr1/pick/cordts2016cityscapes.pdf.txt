The Cityscapes Dataset for Semantic Urban Scene Understanding
Marius Cordts1,2
Markus Enzweiler1
Mohamed Omran3
Rodrigo Benenson3
Sebastian Ramos1,4
Uwe Franke1
Stefan Roth2
Bernt Schiele3
1Daimler AG R&D, 2TU Darmstadt, 3MPI Informatics, 4TU Dresden
www.cityscapes-dataset.net
Abstract
Visual understanding of complex urban street scenes is
an enabling factor for a wide range of applications. Ob-
ject detection has beneﬁted enormously from large-scale
datasets, especially in the context of deep learning. For
semantic urban scene understanding, however, no current
dataset adequately captures the complexity of real-world
urban scenes. To address this, we introduce Cityscapes, a
benchmark suite and large-scale dataset to train and test
approaches for pixel-level and instance-level semantic la-
beling. Cityscapes is comprised of a large, diverse set of
stereo video sequences recorded in streets from 50 different
cities. 5000 of these images have high quality pixel-level
annotations; 20 000 additional images have coarse anno-
tations to enable methods that leverage large volumes of
weakly-labeled data. Crucially, our effort exceeds previ-
ous attempts in terms of dataset size, annotation richness,
scene variability, and complexity. Our accompanying em-
pirical study provides an in-depth analysis of the dataset
characteristics, as well as a performance evaluation of sev-
eral state-of-the-art approaches based on our benchmark.
1. Introduction
Visual scene understanding has moved from an elusive
goal to a focus of much recent research in computer vi-
sion [27]. Semantic reasoning about the contents of a scene
is thereby done on several levels of abstraction. Scene
recognition aims to determine the overall scene category
by putting emphasis on understanding its global properties,
e.g. [46, 82]. Scene labeling methods, on the other hand,
seek to identify the individual constituent parts of a whole
scene as well as their interrelations on a more local pixel-
and instance-level, e.g. [41, 71]. Specialized object-centric
methods fall somewhere in between by focusing on detect-
ing a certain subset of (mostly dynamic) scene constituents,
e.g. [6,12,13,15]. Despite signiﬁcant advances, visual scene
understanding remains challenging, particularly when tak-
ing human performance as a reference.
The resurrection of deep learning [34] has had a major
impact on the current state-of-the-art in machine learning
and computer vision. Many top-performing methods in a
variety of applications are nowadays built around deep neu-
ral networks [30, 41, 66]. A major contributing factor to
their success is the availability of large-scale, publicly avail-
able datasets such as ImageNet [59], PASCAL VOC [14],
PASCAL-Context [45], and Microsoft COCO [38] that al-
low deep neural networks to develop their full potential.
Despite the existing gap to human performance, scene
understanding approaches have started to become essen-
tial components of advanced real-world systems. A par-
ticularly popular and challenging application involves self-
driving cars, which make extreme demands on system
performance and reliability. Consequently, signiﬁcant re-
search efforts have gone into new vision technologies for
understanding complex trafﬁc scenes and driving scenar-
ios [4, 16–18, 58, 62]. Also in this area, research progress
can be heavily linked to the existence of datasets such as
the KITTI Vision Benchmark Suite [19], CamVid [7], Leuven
[35], and Daimler Urban Segmentation [61] datasets. These
urban scene datasets are often much smaller than datasets
addressing more general settings. Moreover, we argue that
they do not fully capture the variability and complexity
of real-world inner-city trafﬁc scenes. Both shortcomings
currently inhibit further progress in visual understanding
of street scenes. To this end, we propose the Cityscapes
benchmark suite and a corresponding dataset, speciﬁcally
1 instance-level annotations are available
2 ignored for evaluation
ﬂat
construction
nature
vehicle
sky
object
human
void
Figure 1. Number of ﬁnely annotated pixels (y-axis) per class and their associated categories (x-axis).
tailored for autonomous driving in an urban environment
and involving a much wider range of highly complex inner-
city street scenes that were recorded in 50 different cities.
Cityscapes signiﬁcantly exceeds previous efforts in terms of
size, annotation richness, and, more importantly, regarding
scene complexity and variability. We go beyond pixel-level
semantic labeling by also considering instance-level seman-
tic labeling in both our annotations and evaluation metrics.
To facilitate research on 3D scene understanding, we also
provide depth information through stereo vision.
Very recently, [75] announced a new semantic scene la-
beling dataset for suburban trafﬁc scenes. It provides tem-
porally consistent 3D semantic instance annotations with
2D annotations obtained through back-projection. We con-
sider our efforts to be complementary given the differences
in the way that semantic annotations are obtained, and in the
type of scenes considered, i.e. suburban vs. inner-city traf-
ﬁc. To maximize synergies between both datasets, a com-
mon label deﬁnition that allows for cross-dataset evaluation
has been mutually agreed upon and implemented.
Designing a large-scale dataset requires a multitude of
decisions, e.g. on the modalities of data recording, data
preparation, and the annotation protocol. Our choices were
guided by the ultimate goal of enabling signiﬁcant progress
in the ﬁeld of semantic urban scene understanding.
2.1. Data speciﬁcations
Our data recording and annotation methodology was
carefully designed to capture the high variability of outdoor
street scenes. Several hundreds of thousands of frames were
acquired from a moving vehicle during the span of several
months, covering spring, summer, and fall in 50 cities, pri-
marily in Germany but also in neighboring countries. We
deliberately did not record in adverse weather conditions,
such as heavy rain or snow, as we believe such conditions
to require specialized techniques and datasets [51].
Our camera system and post-processing reﬂect the cur-
rent state-of-the-art in the automotive domain.
Images
were recorded with an automotive-grade 22 cm baseline
stereo camera using 1/3 in CMOS 2 MP sensors (OnSemi
AR0331) with rolling shutters at a frame-rate of 17 Hz.
The sensors were mounted behind the windshield and yield
high dynamic-range (HDR) images with 16 bits linear color
depth. Each 16 bit stereo image pair was subsequently de-
bayered and rectiﬁed. We relied on [31] for extrinsic and
intrinsic calibration. To ensure calibration accuracy we re-
calibrated on-site before each recording session.
For comparability and compatibility with existing
datasets we also provide low dynamic-range (LDR) 8 bit
RGB images that are obtained by applying a logarithmic
compression curve. Such tone mappings are common in
automotive vision, since they can be computed efﬁciently
and independently for each pixel. To facilitate highest an-
notation quality, we applied a separate tone mapping to each
image. The resulting images are less realistic, but visually
more pleasing and proved easier to annotate. 5000 images
were manually selected from 27 cities for dense pixel-level
annotation, aiming for high diversity of foreground objects,
background, and overall scene layout. The annotations (see
Sec. 2.2) were done on the 20th frame of a 30-frame video
snippet, which we provide in full to supply context informa-
tion. For the remaining 23 cities, a single image every 20 s
or 20 m driving distance (whatever comes ﬁrst) was selected
for coarse annotation, yielding 20 000 images in total.
In addition to the rectiﬁed 16 bit HDR and 8 bit LDR
stereo image pairs and corresponding annotations, our
dataset includes vehicle odometry obtained from in-vehicle
sensors, outside temperature, and GPS tracks.
2.2. Classes and annotations
We provide coarse and ﬁne annotations at pixel level in-
cluding instance-level labels for humans and vehicles.
Our 5000 ﬁne pixel-level annotations consist of layered
polygons (à la LabelMe [60]) and were realized in-house
to guarantee highest quality levels. Annotation and quality
control required more than 1.5 h on average for a single im-
age. Annotators were asked to label the image from back to
front such that no object boundary was marked more than
once. Each annotation thus implicitly provides a depth or-
dering of the objects in the scene. Given our label scheme,
Our dataset
DUS
CamVid
KITTI
ﬂat
construction
nature
vehicle
sky
object
human
void
human
annotations can be easily extended to cover additional or
more ﬁne-grained classes.
For our 20 000 coarse pixel-level annotations, accuracy
on object boundaries was traded off for annotation speed.
We aimed to correctly annotate as many pixels as possible
within a given span of less than 7 min of annotation time per
image. This was achieved by labeling coarse polygons un-
der the sole constraint that each polygon must only include
pixels belonging to a single object class.
In two experiments we assessed the quality of our label-
ing. First, 30 images were ﬁnely annotated twice by dif-
ferent annotators and passed the same quality control.
It
turned out that 96 % of all pixels were assigned to the same
label. Since our annotators were instructed to choose a void
label if unclear (such that the region is ignored in training
and evaluation), we exclude pixels having at least one void
label and recount, yielding 98 % agreement. Second, all our
ﬁne annotations were additionally coarsely annotated such
that we can enable research on densifying coarse labels. We
found that 97 % of all labeled pixels in the coarse annota-
tions were assigned the same class as in the ﬁne annotations.
We deﬁned 30 visual classes for annotation, which are
grouped into eight categories: ﬂat, construction, nature,
vehicle, sky, object, human, and void. Classes were se-
lected based on their frequency, relevance from an applica-
tion standpoint, practical considerations regarding the an-
notation effort, as well as to facilitate compatibility with
existing datasets, e.g. [7, 19, 75]. Classes that are too rare
are excluded from our benchmark, leaving 19 classes for
evaluation, see Fig. 1 for details. We plan to release our
annotation tool upon publication of the dataset.
2.3. Dataset splits
We split our densely annotated images into separate
training, validation, and test sets. The coarsely annotated
images serve as additional training data only. We chose not
to split the data randomly, but rather in a way that ensures
each split to be representative of the variability of different
street scene scenarios. The underlying split criteria involve
a balanced distribution of geographic location and popula-
tion size of the individual cities, as well as regarding the
time of year when recordings took place. Speciﬁcally, each
of the three split sets is comprised of data recorded with the
Ours (ﬁne)
Ours (coarse)
CamVid
DUS
KITTI
Table 1. Absolute number and density of annotated pix-
els for Cityscapes, DUS, KITTI, and CamVid (upscaled to
1280 × 720 pixels to maintain the original aspect ratio).
following properties in equal shares: (i) in large, medium,
and small cities; (ii) in the geographic west, center, and east;
(iii) in the geographic north, center, and south; (iv) at the be-
ginning, middle, and end of the year. Note that the data is
split at the city level, i.e. a city is completely within a sin-
gle split. Following this scheme, we arrive at a unique split
consisting of 2975 training and 500 validation images with
publicly available annotations, as well as 1525 test images
with annotations withheld for benchmarking purposes.
In order to assess how uniform (representative) the splits
are regarding the four split characteristics, we trained a fully
convolutional network [41] on the 500 images in our vali-
dation set. This model was then evaluated on the whole test
set, as well as eight subsets thereof that reﬂect the extreme
values of the four characteristics. With the exception of the
time of year, the performance is very homogeneous, varying
less than 1.5 % points (often much less). Interestingly, the
performance on the end of the year subset is 3.8 % points
better than on the whole test set. We hypothesize that this
is due to softer lighting conditions in the frequently cloudy
fall. To verify this hypothesis, we additionally tested on
images taken in low- or high-temperature conditions, ﬁnd-
ing a 4.5 % point increase in low temperatures (cloudy) and
a 0.9 % point decrease in warm (sunny) weather. More-
over, speciﬁcally training for either condition leads to an
improvement on the respective test set, but not on the bal-
anced set. These ﬁndings support our hypothesis and un-
derline the importance of a dataset covering a wide range of
conditions encountered in the real world in a balanced way.
2.4. Statistical analysis
We compare Cityscapes to other datasets in terms of (i)
annotation volume and density, (ii) the distribution of visual
#humans
#vehicles
Ours (ﬁne)
KITTI
Caltech
Table 2. Absolute and average number of instances for Cityscapes,
KITTI, and Caltech (1 via interpolation) on the respective training
and validation datasets.
classes, and (iii) scene complexity. Regarding the ﬁrst two
aspects, we compare Cityscapes to other datasets with se-
mantic pixel-wise annotations, i.e. CamVid [7], DUS [62],
and KITTI [19]. Note that there are many other datasets
with dense semantic annotations, e.g. [2, 56, 65, 69, 70].
However, we restrict this part of the analysis to those with a
focus on autonomous driving.
CamVid consists of ten minutes of video footage with
pixel-wise annotations for over 700 frames. DUS consists
of a video sequence of 5000 images from which 500 have
been annotated. KITTI addresses several different tasks in-
cluding semantic labeling and object detection. As no of-
ﬁcial pixel-wise annotations exist for KITTI, several inde-
pendent groups have annotated approximately 700 frames
[22, 29, 32, 33, 58, 64, 77, 80]. We map those labels to our
high-level categories and analyze this consolidated set. In
comparison, Cityscapes provides signiﬁcantly more anno-
tated images, i.e. 5000 ﬁne and 20 000 coarse annotations.
Moreover, the annotation quality and richness is notably
better. As Cityscapes provides recordings from 50 differ-
ent cities, it also covers a signiﬁcantly larger area than pre-
vious datasets that contain images from a single city only,
e.g. Cambridge (CamVid), Heidelberg (DUS), and Karl-
sruhe (KITTI). In terms of absolute and relative numbers
of semantically annotated pixels (training, validation, and
test data), Cityscapes compares favorably to CamVid, DUS,
and KITTI with up to two orders of magnitude more anno-
tated pixels, c.f . Tab. 1. The majority of all annotated pixels
in Cityscapes belong to the coarse annotations, providing
many individual (but correlated) training samples, but miss-
ing information close to object boundaries.
Figures 1 and 2 compare the distribution of annotations
across individual classes and their associated higher-level
categories. Notable differences stem from the inherently
different conﬁgurations of the datasets. Cityscapes involves
dense inner-city trafﬁc with wide roads and large intersec-
tions, whereas KITTI is composed of less busy suburban
trafﬁc scenes. As a result, KITTI exhibits signiﬁcantly
fewer ﬂat ground structures, fewer humans, and more na-
ture.
In terms of overall composition, DUS and CamVid
seem more aligned with Cityscapes. Exceptions are an
abundance of sky pixels in CamVid due to cameras with a
comparably large vertical ﬁeld-of-view and the absence of
certain categories in DUS, i.e. nature and object.
Cityscapes
MS COCO
KITTI
Pascal
0
number of trafﬁc participant instances per image
Figure 3. Dataset statistics regarding scene complexity. Only MS
COCO and Cityscapes provide instance segmentation masks.
Our dataset
KITTI
Figure 4. Histogram of object distances in meters for class vehicle.
Finally, we assess scene complexity, where density and
scale of trafﬁc participants (humans and vehicles) serve as
proxy measures. Out of the previously discussed datasets,
only Cityscapes and KITTI provide instance-level annota-
tions for humans and vehicles. We additionally compare
to the Caltech Pedestrian Dataset [12], which only contains
annotations for humans, but none for vehicles. Furthermore,
KITTI and Caltech only provide instance-level annotations
in terms of axis-aligned bounding boxes. We use the respec-
tive training and validation splits for our analysis, since test
set annotations are not publicly available for all datasets.
In absolute terms, Cityscapes contains signiﬁcantly more
object instance annotations than KITTI, see Tab. 2. Be-
ing a specialized benchmark, the Caltech dataset provides
the most annotations for humans by a margin. The major
share of those labels was obtained, however, by interpola-
tion between a sparse set of manual annotations resulting in
signiﬁcantly degraded label quality. The relative statistics
emphasize the much higher complexity of Cityscapes, as
the average numbers of object instances per image notably
exceed those of KITTI and Caltech. We extend our analysis
to MS COCO [38] and PASCAL VOC [14] that also contain
street scenes while not being speciﬁc for them. We analyze
the frequency of scenes with a certain number of trafﬁc par-
ticipant instances, see Fig. 3. We ﬁnd our dataset to cover a
greater variety of scene complexity and to have a higher por-
tion of highly complex scenes than previous datasets. Using
stereo data, we analyze the distribution of vehicle distances
to the camera. From Fig. 4 we observe, that in compar-
ison to KITTI, Cityscapes covers a larger distance range.
We attribute this to both our higher-resolution imagery and
the careful annotation procedure. As a consequence, algo-
rithms need to take a larger range of scales and object sizes
into account to score well in our benchmark.
3. Semantic Labeling
The ﬁrst Cityscapes task involves predicting a per-pixel
semantic labeling of the image without considering higher-
level object instance or boundary information.
3.1. Tasks and metrics
To assess labeling performance, we rely on a standard
and a novel metric. The ﬁrst is the standard Jaccard Index,
commonly known as the PASCAL VOC intersection-over-
TP+FP+FN [14], where TP, FP, and FN
union metric IoU =
are the numbers of true positive, false positive, and false
negative pixels, respectively, determined over the whole test
set. Owing to the two semantic granularities, i.e. classes
and categories, we report two separate mean performance
scores: IoUcategory and IoUclass. In either case, pixels labeled
as void do not contribute to the score.
iTP
The global IoU measure is biased toward object in-
stances that cover a large image area. In street scenes with
their strong scale variation this can be problematic. Specif-
ically for trafﬁc participants, which are the key classes in
our scenario, we aim to evaluate how well the individual
instances in the scene are represented in the labeling. To
address this, we additionally evaluate the semantic label-
ing using an instance-level intersection-over-union metric
iTP+FP+iFN. Here, iTP, and iFN denote weighted
iIoU =
counts of true positive and false negative pixels, respec-
tively.
In contrast to the standard IoU measure, the con-
tribution of each pixel is weighted by the ratio of the class’
average instance size to the size of the respective ground
truth instance. As before, FP is the number of false positive
pixels. It is important to note here that unlike the instance-
level task in Sec. 4, we assume that the methods only yield a
standard per-pixel semantic class labeling as output. There-
fore, the false positive pixels are not associated with any
instance and thus do not require normalization. The ﬁnal
scores, iIoUcategory and iIoUclass, are obtained as the means
for the two semantic granularities, while only classes with
instance annotations are included.
3.2. Control experiments
We conduct several control experiments to put our base-
line results below into perspective. First, we count the rela-
tive frequency of every class label at each pixel location of
the ﬁne (coarse) training annotations. Using the most fre-
quent label at each pixel as a constant prediction irrespective
of the test image (called static ﬁne, SF, and static coarse,
SC) results in roughly 10 % IoUclass, as shown in Tab. 3.
These low scores emphasize the high diversity of our data.
SC and SF having similar performance indicates the value
of our additional coarse annotations. Even if the ground
truth (GT) segments are re-classiﬁed using the most fre-
quent training label (SF or SC) within each segment mask,
the performance does not notably increase.
Secondly, we re-classify each ground truth segment us-
ing FCN-8s [41], c.f . Sec. 3.4. We compute the average
scores within each segment and assign the maximizing la-
bel. The performance is signiﬁcantly better than the static
predictors but still far from 100 %. We conclude that it is
necessary to optimize both classiﬁcation and segmentation
quality at the same time.
Thirdly, we evaluate the performance of subsampled
ground truth annotations as predictors. Subsampling was
done by majority voting of neighboring pixels, followed
by resampling back to full resolution. This yields an up-
per bound on the performance at a ﬁxed output resolution
and is particularly relevant for deep learning approaches
that often apply downscaling due to constraints on time,
memory, or the network architecture itself. Downsampling
factors 2 and 4 correspond to the most common setting of
our 3rd-party baselines (Sec. 3.4). Note that while subsam-
pling by a factor of 2 hardly affects the IoU score, it clearly
decreases the iIoU score given its comparatively large im-
pact on small, but nevertheless important objects. This un-
derlines the importance of the separate instance-normalized
evaluation. The downsampling factors of 8, 16, and 32 are
motivated by the corresponding strides of the FCN model.
The performance of a GT downsampling by a factor of 64 is
comparable to the current state of the art, while downsam-
pling by a factor of 128 is the smallest (power of 2) down-
sampling for which all images have a distinct labeling.
Lastly, we employ 128-times subsampled annotations
and retrieve the nearest training annotation in terms of the
Hamming distance. The full resolution version of this train-
ing annotation is then used as prediction, resulting in 21 %
IoUclass. While outperforming the static predictions, the
poor result demonstrates the high variability of our dataset
and its demand for approaches that generalize well.
Drawing on the success of deep learning algorithms, a
number of semantic labeling approaches have shown very
promising results and signiﬁcantly advanced the state of
the art. These new approaches take enormous advantage
from recently introduced large-scale datasets, e.g. PASCAL-
Context [45] and Microsoft COCO [38]. Cityscapes aims
to complement these, particularly in the context of under-
standing complex urban scenarios, in order to enable further
research in this area.
The popular work of Long et al. [41] showed how a top-
performing Convolutional Neural Network (CNN) for im-
age classiﬁcation can be successfully adapted for the task
of semantic labeling. Following this line, [9, 37, 40, 63, 81]
propose different approaches that combine the strengths of
CNNs and Conditional Random Fields (CRFs).
Other work takes advantage of deep learning for ex-
plicitly integrating global scene context in the prediction
Average over
Metric [%]
static ﬁne (SF)
static coarse (SC)
GT segmentation with SF
GT segmentation with SC
GT segmentation with [41]
GT subsampled by 2
GT subsampled by 4
GT subsampled by 8
GT subsampled by 16
GT subsampled by 32
GT subsampled by 64
GT subsampled by 128
nearest training neighbor
Classes
IoU
iIoU
Categories
iIoU
IoU
Table 3. Quantitative results of control experiments for semantic
labeling using the metrics presented in Sec. 3.1.
Classes
IoU
iIoU
Categories
IoU
iIoU
Table 4. Quantitative results of baselines for semantic labeling us-
ing the metrics presented in Sec. 3.1. The ﬁrst block lists results
from our own experiments, the second from those provided by 3rd
parties. All numbers are given in percent and we indicate the used
training data for each method, i.e. train ﬁne, val ﬁne, coarse extra
as well as a potential downscaling factor (sub) of the input image.
of pixel-wise semantic labels, in particular through CNNs
[4,39,44,67] or Recurrent Neural Networks (RNNs) [8,52].
Furthermore, a novel CNN architecture explicitly designed
for dense prediction has been proposed recently by [79].
Last but not least, several studies [5,11,48–50,53,74,76]
lately have explored different forms of weak supervision,
such as bounding boxes or image-level labels, for training
CNNs for pixel-level semantic labeling. We hope our coarse
annotations can further advance this area.
Our own baseline experiments (Tab. 4, top) rely on fully
convolutional networks (FCNs), as they are central to most
state-of-the-art methods [9, 37, 41, 63, 81]. We adopted
VGG16 [68] and utilize the PASCAL-context setup [41]
with a modiﬁed learning rate to match our image resolu-
tion under an unnormalized loss. According to the notation
in [41], we denote the different models as FCN-32s, FCN-
16s, and FCN-8s, where the numbers are the stride of the
ﬁnest heatmap. Since VGG16 training on 2 MP images ex-
ceeds even the largest GPU memory available, we split each
image into two halves with sufﬁciently large overlap. Ad-
ditionally, we trained a model on images downscaled by a
factor of 2. We ﬁrst train on our training set (train) until the
performance on our validation set (val) saturates, and then
retrain on train+val with the same number of epochs.
To obtain further baseline results, we asked selected
groups that have proposed state-of-the-art semantic label-
ing approaches to optimize their methods on our dataset
and evaluated their predictions on our test set. The resulting
scores are given in Tab. 4 (bottom) and qualitative exam-
ples of three selected methods are shown in Fig. 5. Interest-
ingly enough, the performance ranking in terms of the main
IoUclass score on Cityscapes is highly different from PAS-
CAL VOC [14]. While DPN [40] is the 2nd best method
on PASCAL, it is only the 6th best on Cityscapes. FCN-
8s [41] is last on PASCAL, but 3rd best on Cityscapes. Ade-
laide [37] performs consistently well on both datasets with
rank 1 on PASCAL and 2 on Cityscapes.
From studying these results, we draw several conclu-
sions: (1) The amount of downscaling applied during train-
ing and testing has a strong and consistent negative inﬂu-
ence on performance (c.f . FCN-8s vs. FCN-8s at half res-
olution, as well as the 2nd half of the table). The ranking
according to IoUclass is strictly consistent with the degree
of downscaling. We attribute this to the large scale vari-
ation present in our dataset, c.f . Fig. 4. This observation
clearly indicates the demand for additional research in the
direction of memory and computationally efﬁcient CNNs
when facing such a large-scale dataset with high-resolution
images. (2) Our novel iIoU metric treats instances of any
size equally and is therefore more sensitive to errors in
predicting small objects compared to the IoU. Methods
that leverage a CRF for regularization [9, 40, 48, 81] tend
to over smooth small objects, c.f . Fig. 5, hence show a
larger drop from IoU to iIoU than [4] or FCN-8s [41]. [37]
is the only exception; its speciﬁc FCN-derived pairwise
terms apparently allow for a more selective regularization.
(3) When considering IoUcategory, Dilated10 [79] and FCN-
8s [41] perform particularly well, indicating that these ap-
proaches produce comparatively many confusions between
the classes within the same category, c.f . the buses in Fig. 5
(top). (4) Training FCN-8s [41] with 500 densely annotated
Best reported result Our result
Table 5. Quantitative results (avg. recall in percent) of
our half-resolution FCN-8s model trained on Cityscapes
images and tested on Camvid and KITTI.
images (750 h of annotation) yields comparable IoU perfor-
mance to a model trained on 20 000 weakly annotated im-
ages (1300 h annot.), c.f . rows 5 & 6 in Tab. 4. However, in
both cases the performance is signiﬁcantly lower than FCN-
8s trained on all 3475 densely annotated images. Many ﬁne
labels are thus important for training standard methods as
well as for testing, but the performance using coarse annota-
tions only does not collapse and presents a viable option. (5)
Since the coarse annotations do not include small or distant
instances, their iIoU performance is worse. (6) Coarse la-
bels can complement the dense labels if applying appropri-
ate methods as evidenced by [48] outperforming [9], which
it extends by exploiting both dense and weak annotations
(e.g. bounding boxes). Our dataset will hopefully stimulate
research on exploiting the coarse labels further, especially
given the interest in this area, e.g. [25, 43, 47].
Overall, we believe that the unique characteristics of our
dataset (e.g. scale variation, amount of small objects, focus
on urban street scenes) allow for more such novel insights.
3.5. Cross-dataset evaluation
In order to show the compatibility and complementarity
of Cityscapes regarding related datasets, we applied an FCN
model trained on our data to Camvid [7] and two subsets of
KITTI [58, 64]. We use the half-resolution model (c.f . 4th
row in Tab. 4) to better match the target datasets, but we do
not apply any speciﬁc training or ﬁne-tuning. In all cases,
we follow the evaluation of the respective dataset to be able
to compare to previously reported results [4, 73]. The ob-
tained results in Tab. 5 show that our large-scale dataset
enables us to train models that are on a par with or even
outperforming methods that are speciﬁcally trained on an-
other benchmark and specialized for its test data. Further,
our analysis shows that our new dataset integrates well with
existing ones and allows for cross-dataset research.
4. Instance-Level Semantic Labeling
The pixel-level task, c.f . Sec. 3, does not aim to segment
individual object instances.
In contrast, in the instance-
level semantic labeling task, we focus on simultaneously
detecting objects and segmenting them. This is an exten-
sion to both traditional object detection, since per-instance
segments must be provided, and semantic labeling, since
each instance is treated as a separate label.
For instance-level semantic labeling, algorithms are re-
quired to deliver a set of detections of trafﬁc participants
in the scene, each associated with a conﬁdence score and
a per-instance segmentation mask. To assess instance-level
performance, we compute the average precision on the re-
gion level (AP [23]) for each class and average it across a
range of overlap thresholds to avoid a bias towards a spe-
ciﬁc value. Speciﬁcally, we follow [38] and use 10 different
overlaps ranging from 0.5 to 0.95 in steps of 0.05. The
overlap is computed at the region level, making it equiva-
lent to the IoU of a single instance. We penalize multiple
predictions of the same ground truth instance as false posi-
tives. To obtain a single, easy to compare compound score,
we report the mean average precision AP, obtained by also
averaging over the class label set. As minor scores, we add
AP50% for an overlap value of 50 %, as well as AP100m and
AP50m where the evaluation is restricted to objects within
100 m and 50 m distance, respectively.
4.2. State of the art
As detection results have matured (70 % mean AP on
PASCAL [14, 55]), the last years have seen a rising inter-
est in more difﬁcult settings. Detections with pixel-level
segments rather than traditional bounding boxes provide a
richer output and allow (in principle) for better occlusion
handling. We group existing methods into three categories.
The ﬁrst encompasses segmentation, then detection and
most prominently the R-CNN detection framework [21], re-
lying on object proposals for generating detections. Many
of the commonly used bounding box proposal methods
[28, 54] ﬁrst generate a set of overlapping segments, e.g.
Selective Search [72] or MCG [1]. In R-CNN, bounding
boxes of each segment are then scored using a CNN-based
classiﬁer, while each segment is treated independently.
The second category encompasses detection, then seg-
mentation, where bounding-box detections are reﬁned to
instance speciﬁc segmentations. Either CNNs [23, 24] or
non-parametric methods [10] are typically used, however,
in both cases without coupling between individual predic-
tions.
Third, simultaneous detection and segmentation is sig-
niﬁcantly more delicate. Earlier methods relied on Hough
voting [36, 57]. More recent works formulate a joint in-
ference problem on pixel and instance level using CRFs
[11, 26, 42, 71, 78, 80]. Differences lie in the generation
of proposals (exemplars, average class shape, direct regres-
sion), the cues considered (pixel-level labeling, depth order-
ing), and the inference method (probabilistic, heuristics).
4.3. Lower bounds, oracles & baselines
In Tab. 6, we provide lower-bounds that any sensible
method should improve upon, as well as oracle-case results
Figure 5. Qualitative examples of selected baselines. From left to right: image with stereo depth maps partially overlayed, annotation,
DeepLab [48], Adelaide [37], and Dilated10 [79]. The color coding of the semantic classes matches Fig. 1.
5. Conclusion and Outlook
In this work, we presented Cityscapes, a comprehensive
benchmark suite that has been carefully designed to spark
progress in semantic urban scene understanding by: (i) cre-
ating the largest and most diverse dataset of street scenes
with high-quality and coarse annotations to date; (ii) devel-
oping a sound evaluation methodology for pixel-level and
instance-level semantic labeling; (iii) providing an in-depth
analysis of the characteristics of our dataset; (iv) evaluating
several state-of-the-art approaches on our benchmark. To
keep pace with the rapid progress in scene understanding,
we plan to adapt Cityscapes to future needs over time.
The signiﬁcance of Cityscapes is all the more apparent
from three observations. First, the relative order of perfor-
mance for state-of-the-art methods on our dataset is notably
different than on more generic datasets such as PASCAL
VOC. Our conclusion is that serious progress in urban scene
understanding may not be achievable through such generic
datasets. Second, the current state-of-the-art in semantic la-
beling on KITTI and CamVid is easily reached and to some
extent even outperformed by applying an off-the-shelf fully-
convolutional network [41] trained on Cityscapes only, as
demonstrated in Sec. 3.5. This underlines the compatibil-
ity and unique beneﬁt of our dataset. Third, Cityscapes will
pose a signiﬁcant new challenge for our ﬁeld given that it is
currently far from being solved. The best performing base-
line for pixel-level semantic segmentation obtains an IoU
score of 67.1 %, whereas the best current methods on PAS-
CAL VOC and KITTI reach IoU levels of 77.9 % [3] and
72.5 % [73], respectively.
In addition, the instance-level
task is particularly challenging with an AP score of 4.6 %.
Acknowledgments. S. Roth was supported in part by the Euro-
pean Research Council under the EU’s 7th Framework Programme
(FP/2007-2013)/ERC Grant agreement no. 307942. The authors
acknowledge the support of the Bundesministerium für Wirtschaft
und Technologie (BMWi) in the context of the UR:BAN initiative.
We thank the 3rd-party authors for their valuable submissions.
Proposals
MCG regions
MCG bboxes
MCG hulls
GT bboxes
GT regions
MCG regions GT
GT
MCG bboxes
MCG hulls
GT
Table 6. Baseline results on instance-level semantic labeling task
using the metrics described in Sec. 4. All numbers in %.
(i.e. using the test time ground truth). For our experiments,
we rely on publicly available implementations. We train a
Fast-R-CNN (FRCN) detector [20] on our training data in
order to score MCG object proposals [1]. Then, we use
either its output bounding boxes as (rectangular) segmen-
tations, the associated region proposal, or its convex hull
as a per-instance segmentation. The best main score AP is
4.6 %, is obtained with convex hull proposals, and becomes
larger when restricting the evaluation to 50 % overlap or
close instances. We contribute these rather low scores to
our challenging dataset, biased towards busy and cluttered
scenes, where many, often highly occluded, objects occur
at various scales, c.f . Sec. 2. Further, the MCG bottom-up
proposals seem to be unsuited for such street scenes and
cause extremely low scores when requiring large overlaps.
We conﬁrm this interpretation with oracle experiments,
where we replace the proposals at test-time with ground
truth segments or replace the FRCN classiﬁer with an or-
acle. In doing so, the task of object localization is decou-
pled from the classiﬁcation task. The results in Tab. 6 show
that when bound to MCG proposals, the oracle classiﬁer is
only slightly better than FRCN. On the other hand, when the
proposals are perfect, FRCN achieves decent results. Over-
all, these observations unveil that the instance-level perfor-
mance of our baseline is bound by the region proposals.
segmentation with occlusion handling. In CVPR, 2015. 7
[17] P. Furgale, U. Schwesinger, M. Ruﬂi, W. Derendarz,
H. Grimmett, P. Mühlfellner, S. Wonneberger, B. Li, et al.
Toward automated driving in cities using close-to-market
sensors: An overview of the V-Charge project. In IV Sympo-
sium, 2013. 1
[29] H. Hu and B. Upcroft. Nonparametric semantic segmenta-
ImageNet
In
classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 1
In-
factory calibration of multiocular camera systems. In SPIE
Photonics Europe (Optical Metrology in Production Engi-
neering), 2004. 2
[44] M. Mostajabi, P. Yadollahpour, and G. Shakhnarovich. Feed-
forward semantic segmentation with zoom-out features. In
CVPR, 2015. 6
[49] D. Pathak, P. Kraehenbuehl, and T. Darrell. Constrained con-
volutional neural networks for weakly supervised segmenta-
tion. In ICCV, 2015. 6
[52] P. H. Pinheiro and R. Collobert. Recurrent convolutional
[56] H. Riemenschneider, A. Bódis-Szomorú, J. Weissenberg,
and L. Van Gool. Learning where to classify in multi-view
semantic segmentation. In ECCV. 2014. 4
ﬁcient multi-cue scene segmentation.
3
[63] A. Schwing and R. Urtasun. Fully connected deep structured
[66] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and
Y. LeCun. OverFeat: Integrated recognition, localization and
detection using convolutional networks. In ICLR, 2014. 1
Very deep con-
large-scale image recognition.
RGB-D scene understanding benchmark suite.
2015. 4
[78] J. Yao, S. Fidler, and R. Urtasun. Describing the scene as
a whole: Joint object detection, scene classiﬁcation and se-
mantic segmentation. In CVPR, 2012. 7
