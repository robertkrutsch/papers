Multi-Task Learning Using Uncertainty to Weigh Losses
for Scene Geometry and Semantics
Alex Kendall
University of Cambridge
agk34@cam.ac.uk
Yarin Gal
University of Oxford
yarin@cs.ox.ac.uk
Roberto Cipolla
University of Cambridge
rc10001@cam.ac.uk
Abstract
Numerous deep learning applications beneﬁt from multi-
task learning with multiple regression and classiﬁcation ob-
jectives.
In this paper we make the observation that the
performance of such systems is strongly dependent on the
relative weighting between each task’s loss. Tuning these
weights by hand is a difﬁcult and expensive process, mak-
ing multi-task learning prohibitive in practice. We pro-
pose a principled approach to multi-task deep learning
which weighs multiple loss functions by considering the ho-
moscedastic uncertainty of each task. This allows us to si-
multaneously learn various quantities with different units
or scales in both classiﬁcation and regression settings. We
demonstrate our model learning per-pixel depth regression,
semantic and instance segmentation from a monocular in-
put image. Perhaps surprisingly, we show our model can
learn multi-task weightings and outperform separate mod-
els trained individually on each task.
1. Introduction
Multi-task learning aims to improve learning efﬁciency
and prediction accuracy by learning multiple objectives
from a shared representation [7]. Multi-task learning is
prevalent in many applications of machine learning – from
computer vision [27] to natural language processing [11] to
speech recognition [23].
We explore multi-task learning within the setting of vi-
sual scene understanding in computer vision. Scene under-
standing algorithms must understand both the geometry and
semantics of the scene at the same time. This forms an in-
teresting multi-task learning problem because scene under-
standing involves joint learning of various regression and
classiﬁcation tasks with different units and scales. Multi-
task learning of visual scene understanding is of crucial
importance in systems where long computation run-time is
prohibitive, such as the ones used in robotics. Combining
all tasks into a single model reduces computation and allows
these systems to run in real-time.
Prior approaches to simultaneously learning multiple
tasks use a na¨ıve weighted sum of losses, where the loss
weights are uniform, or manually tuned [38, 27, 15]. How-
ever, we show that performance is highly dependent on an
appropriate choice of weighting between each task’s loss.
Searching for an optimal weighting is prohibitively expen-
sive and difﬁcult to resolve with manual tuning. We observe
that the optimal weighting of each task is dependent on the
measurement scale (e.g. meters, centimetres or millimetres)
and ultimately the magnitude of the task’s noise.
In this work we propose a principled way of combining
multiple loss functions to simultaneously learn multiple ob-
jectives using homoscedastic uncertainty. We interpret ho-
moscedastic uncertainty as task-dependent weighting and
show how to derive a principled multi-task loss function
which can learn to balance various regression and classiﬁca-
tion losses. Our method can learn to balance these weight-
ings optimally, resulting in superior performance, compared
with learning each task individually.
Speciﬁcally, we demonstrate our method in learning
scene geometry and semantics with three tasks. Firstly, we
learn to classify objects at a pixel level, also known as se-
mantic segmentation [32, 3, 42, 8, 45]. Secondly, our model
performs instance segmentation, which is the harder task of
segmenting separate masks for each individual object in an
image (for example, a separate, precise mask for each in-
dividual car on the road) [37, 18, 14, 4]. This is a more
difﬁcult task than semantic segmentation, as it requires not
only an estimate of each pixel’s class, but also which object
that pixel belongs to. It is also more complicated than ob-
ject detection, which often predicts object bounding boxes
alone [17]. Finally, our model predicts pixel-wise metric
depth. Depth by recognition has been demonstrated using
dense prediction networks with supervised [15] and unsu-
pervised [16] deep learning. However it is very hard to esti-
mate depth in a way which generalises well. We show that
we can improve our estimation of geometry and depth by
using semantic labels and multi-task deep learning.
In existing literature, separate deep learning models
Figure 1: Multi-task deep learning. We derive a principled way of combining multiple regression and classiﬁcation loss functions for
multi-task learning. Our architecture takes a single monocular RGB image as input and produces a pixel-wise classiﬁcation, an instance
semantic segmentation and an estimate of per pixel depth. Multi-task learning can improve accuracy over separately trained models because
cues from one task, such as depth, are used to regularize and improve the generalization of another domain, such as segmentation.
would be used to learn depth regression, semantic segmen-
tation and instance segmentation to create a complete scene
understanding system. Given a single monocular input im-
age, our system is the ﬁrst to produce a semantic segmenta-
tion, a dense estimate of metric depth and an instance level
segmentation jointly (Figure 1). While other vision mod-
els have demonstrated multi-task learning, we show how to
learn to combine semantics and geometry. Combining these
tasks into a single model ensures that the model agrees be-
tween the separate task outputs while reducing computa-
tion. Finally, we show that using a shared representation
with multi-task learning improves performance on various
metrics, making the models more effective.
In summary, the key contributions of this paper are:
1. a novel and principled multi-task loss to simultane-
ously learn various classiﬁcation and regression losses
of varying quantities and units using homoscedastic
task uncertainty,
2. a uniﬁed architecture for semantic segmentation, in-
stance segmentation and depth regression,
3. demonstrating the importance of loss weighting in
multi-task deep learning and how to obtain superior
performance compared to equivalent separately trained
models.
Multi-task learning aims to improve learning efﬁciency
and prediction accuracy for each task, when compared to
training a separate model for each task [40, 5]. It can be con-
sidered an approach to inductive knowledge transfer which
improves generalisation by sharing the domain information
between complimentary tasks. It does this by using a shared
representation to learn multiple tasks – what is learned from
one task can help learn other tasks [7].
Fine-tuning [1, 36] is a basic example of multi-task
learning, where we can leverage different learning tasks by
considering them as a pre-training step. Other models al-
ternate learning between each training task, for example in
natural language processing [11]. Multi-task learning can
also be used in a data streaming setting [40], or to prevent
forgetting previously learned tasks in reinforcement learn-
ing [26]. It can also be used to learn unsupervised features
from various data sources with an auto-encoder [35].
In computer vision there are many examples of methods
for multi-task learning. Many focus on semantic tasks, such
as classiﬁcation and semantic segmentation [30] or classiﬁ-
cation and detection [38]. MultiNet [39] proposes an archi-
tecture for detection, classiﬁcation and semantic segmenta-
tion. CrossStitch networks [34] explore methods to com-
bine multi-task neural activations. Uhrig et al. [41] learn
semantic and instance segmentations under a classiﬁcation
setting. Multi-task deep learning has also been used for ge-
ometry and regression tasks.
[15] show how to learn se-
mantic segmentation, depth and surface normals. PoseNet
[25] is a model which learns camera position and orienta-
tion. UberNet [27] learns a number of different regression
and classiﬁcation tasks under a single architecture. In this
work we are the ﬁrst to propose a method for jointly learn-
ing depth regression, semantic and instance segmentation.
Like the model of [15], our model learns both semantic and
geometry representations, which is important for scene un-
derstanding. However, our model learns the much harder
task of instance segmentation which requires knowledge of
both semantics and geometry. This is because our model
must determine the class and spatial relationship for each
pixel in each object for instance segmentation.
Classiﬁcation Weight
Classiﬁcation
Classiﬁcation
Depth Regression
Depth Weight
Learned weights
with task uncertainty
(this work, Section 3.2)
(a) Comparing loss weightings when learning semantic classiﬁcation and depth regression
Instance
Depth
Err. [px] Err. [px]
Task Weights
Instance
Instance Regression
Instance Regression
Depth Regression
Depth Weight
with task uncertainty
(this work, Section 3.2)
(b) Comparing loss weightings when learning instance regression and depth regression
Figure 2: Learning multiple tasks improves the model’s representation and individual task performance. These ﬁgures and tables
illustrate the advantages of multi-task learning for (a) semantic classiﬁcation and depth regression and (b) instance and depth regression.
Performance of the model in individual tasks is seen at both edges of the plot where w = 0 and w = 1. For some balance of weightings
between each task, we observe improved performance for both tasks. All models were trained with a learning rate of 0.01 with the
respective weightings applied to the losses using the loss function in (1). Results are shown using the Tiny CityScapes validation dataset
using a down-sampled resolution of 128 × 256.
More importantly, all previous methods which learn mul-
tiple tasks simultaneously use a na¨ıve weighted sum of
losses, where the loss weights are uniform, or crudely and
manually tuned.
In this work we propose a principled
way of combining multiple loss functions to simultaneously
learn multiple objectives using homoscedastic task uncer-
tainty. We illustrate the importance of appropriately weight-
ing each task in deep learning to achieve good performance
and show that our method can learn to balance these weight-
ings optimally.
3. Multi Task Learning with Homoscedastic
Uncertainty
Multi-task learning concerns the problem of optimising a
model with respect to multiple objectives. It is prevalent in
many deep learning problems. The naive approach to com-
bining multi objective losses would be to simply perform a
weighted linear sum of the losses for each individual task:
This is the dominant approach used by prior work [39,
38, 30, 41], for example for dense prediction tasks [27],
for scene understanding tasks [15] and for rotation (in
quaternions) and translation (in meters) for camera pose
[25]. However, there are a number of issues with this
method. Namely, model performance is extremely sensitive
to weight selection, wi, as illustrated in Figure 2. These
weight hyper-parameters are expensive to tune, often taking
many days for each trial. Therefore, it is desirable to ﬁnd a
more convenient approach which is able to learn the optimal
weights.
More concretely, let us consider a network which learns
to predict pixel-wise depth and semantic class from an in-
put image. In Figure 2 the two boundaries of each plot show
models trained on individual tasks, with the curves showing
performance for varying weights wi for each task. We ob-
serve that at some optimal weighting, the joint network per-
forms better than separate networks trained on each task in-
dividually (performance of the model in individual tasks is
seen at both edges of the plot: w = 0 and w = 1). At near-
by values to the optimal weight the network performs worse
on one of the tasks. However, searching for these optimal
weightings is expensive and increasingly difﬁcult with large
models with numerous tasks. Figure 2 also shows a similar
result for two regression tasks; instance segmentation and
depth regression. We next show how to learn optimal task
weightings using ideas from probabilistic modelling.
3.1. Homoscedastic uncertainty as task-dependent
uncertainty
In Bayesian modelling, there are two main types of un-
• Epistemic uncertainty is uncertainty in the model,
which captures what our model does not know due to
lack of training data. It can be explained away with
increased training data.
• Aleatoric uncertainty captures our uncertainty with re-
spect to information which our data cannot explain.
Aleatoric uncertainty can be explained away with the
ability to observe all explanatory variables with in-
creasing precision.
Aleatoric uncertainty can again be divided into two sub-
categories.
• Data-dependent or Heteroscedastic uncertainty is
aleatoric uncertainty which depends on the input data
and is predicted as a model output.
• Task-dependent or Homoscedastic uncertainty is
aleatoric uncertainty which is not dependent on the in-
put data. It is not a model output, rather it is a quantity
which stays constant for all input data and varies be-
tween different tasks. It can therefore be described as
task-dependent uncertainty.
In a multi-task setting, we show that the task uncertainty
captures the relative conﬁdence between tasks, reﬂecting
the uncertainty inherent to the regression or classiﬁcation
task. It will also depend on the task’s representation or unit
of measure. We propose that we can use homoscedastic
uncertainty as a basis for weighting losses in a multi-task
learning problem.
3.2. Multi-task likelihoods
In this section we derive a multi-task loss function based
on maximising the Gaussian likelihood with homoscedastic
uncertainty. Let f W(x) be the output of a neural network
with weights W on input x. We deﬁne the following proba-
bilistic model. For regression tasks we deﬁne our likelihood
as a Gaussian with mean given by the model output:
with an observation noise scalar σ. For classiﬁcation we
often squash the model output through a softmax function,
and sample from the resulting probability vector:
In the case of multiple model outputs, we often deﬁne the
likelihood to factorise over the outputs, given some sufﬁ-
cient statistics. We deﬁne f W(x) as our sufﬁcient statistics,
and obtain the following multi-task likelihood:
p(y1, ..., yK|f W(x)) = p(y1|f W(x))...p(yK|f W(x))
(4)
with model outputs y1, ..., yK (such as semantic segmenta-
tion, depth regression, etc).
In maximum likelihood inference, we maximise the log
likelihood of the model. In regression, for example, the log
likelihood can be written as
log p(y|f W(x)) ∝ − 1
for a Gaussian likelihood (or similarly for a Laplace like-
lihood) with σ the model’s observation noise parameter –
capturing how much noise we have in the outputs. We then
maximise the log likelihood with respect to the model pa-
rameters W and observation noise parameter σ.
We interpret minimising this last objective with respect
to σ1 and σ2 as learning the relative weight of the losses
L1(W) and L2(W) adaptively, based on the data. As σ1
– the noise parameter for the variable y1 – increases, we
have that the weight of L1(W) decreases. On the other
hand, as the noise decreases, we have that the weight of
the respective objective increases. The noise is discouraged
from increasing too much (effectively ignoring the data) by
the last term in the objective, which acts as a regulariser for
the noise terms.
This construction can be trivially extended to multiple
regression outputs. However, the extension to classiﬁcation
likelihoods is more interesting. We adapt the classiﬁcation
likelihood to squash a scaled version of the model output
through a softmax function:
with a positive scalar σ. This can be interpreted as a Boltz-
mann distribution (also called Gibbs distribution) where the
input is scaled by σ2 (often referred to as temperature). This
scalar is either ﬁxed or can be learnt, where the parameter’s
magnitude determines how ‘uniform’ (ﬂat) the discrete dis-
tribution is. This relates to its uncertainty, as measured in
entropy. The log likelihood for this output can then be writ-
ten as
log p(y = c|f W(x), σ) =
exp
− log Softmax(y2, f W(x)) for the cross entropy loss of y2
(with f W(x) not scaled), and optimise with respect to W
as well as σ1, σ2. In the last transition we introduced the ex-
≈
plicit simplifying assumption 1
σ2
σ2
2 which becomes an equality
when σ2 → 1. This has the advantage of simplifying the
optimisation objective, as well as empirically improving re-
sults.
This last objective can be seen as learning the relative
weights of the losses for each output. Large scale values
σ2 will decrease the contribution of L2(W), whereas small
scale σ2 will increase its contribution. The scale is regulated
by the last term in the equation. The objective is penalised
when setting σ2 too large.
This construction can be trivially extended to arbitrary
combinations of discrete and continuous loss functions, al-
lowing us to learn the relative weights of each loss in a
principled and well-founded way. This loss is smoothly dif-
ferentiable, and is well formed such that the task weights
will not converge to zero. In contrast, directly learning the
weights using a simple linear sum of losses (1) would result
in weights which quickly converge to zero. In the following
sections we introduce our experimental model and present
empirical results.
In practice, we train the network to predict the log vari-
ance, s := log σ2. This is because it is more numerically
stable than regressing the variance, σ2, as the loss avoids
any division by zero. The exponential mapping also allows
us to regress unconstrained scalar values, where exp(−s)
is resolved to the positive domain giving valid values for
variance.
4. Scene Understanding Model
To understand semantics and geometry we ﬁrst propose
an architecture which can learn regression and classiﬁcation
outputs, at a pixel level. Our architecture is a deep con-
volutional encoder decoder network [3]. Our model con-
sists of a number of convolutional encoders which produce
a shared representation, followed by a corresponding num-
ber of task-speciﬁc convolutional decoders. A high level
summary is shown in Figure 1.
The purpose of the encoder is to learn a deep mapping to
produce rich, contextual features, using domain knowledge
from a number of related tasks. Our encoder is based on
DeepLabV3 [10], which is a state of the art semantic seg-
mentation framework. We use ResNet101 [20] as the base
feature encoder, followed by an Atrous Spatial Pyramid
Pooling (ASPP) module [10] to increase contextual aware-
ness. We apply dilated convolutions in this encoder, such
that the resulting feature map is sub-sampled by a factor of
(b) Semantic Segmentation
(b) Instance Segmentation
(c) Instance vector regression
(d) Instance Segmentation
Figure 3: Instance centroid regression method. For each pixel,
we regress a vector pointing to the instance’s centroid. The loss is
only computed over pixels which are from instances. We visualise
(c) by representing colour as the orientation of the instance vector,
and intensity as the magnitude of the vector.
8 compared to the input image dimensions.
We then split the network into separate decoders (with
separate weights) for each task. The purpose of the decoder
is to learn a mapping from the shared features to an output.
Each decoder consists of a 3 × 3 convolutional layer with
output feature size 256, followed by a 1× 1 layer regressing
the task’s output. Further architectural details are described
in Appendix A.
Semantic Segmentation. We use the cross-entropy loss
to learn pixel-wise class probabilities, averaging the loss
over the pixels with semantic labels in each mini-batch.
Instance Segmentation. An intuitive method for deﬁn-
ing which instance a pixel belongs to is an association to the
instance’s centroid. We use a regression approach for in-
stance segmentation [29]. This approach is inspired by [28]
which identiﬁes instances using Hough votes from object
parts. In this work we extend this idea by using votes from
individual pixels using deep learning. We learn an instance
vector, ˆxn, for each pixel coordinate, cn, which points to the
centroid of the pixel’s instance, in, such that in = ˆxn + cn.
We train this regression with an L1 loss using ground truth
labels xn, averaged over all labelled pixels, NI, in a mini-
batch: LInstance = 1|NI|
Figure 3 details the representation we use for instance
segmentation. Figure 3(a) shows the input image and a
mask of the pixels which are of an instance class (at test
time inferred from the predicted semantic segmentation).
Figure 3(b) and Figure 3(c) show the ground truth and pre-
dicted instance vectors for both x and y coordinates. We
then cluster these votes using OPTICS [2], resulting in the
predicted instance segmentation output in Figure 3(d).
One of the most difﬁcult cases for instance segmentation
algorithms to handle is when the instance mask is split due
Figure 4: This example shows two cars which are occluded by
trees and lampposts, making the instance segmentation challeng-
ing. Our instance segmentation method can handle occlusions ef-
fectively. We can correctly handle segmentation masks which are
split by occlusion, yet part of the same instance, by incorporating
semantics and geometry.
to occlusion. Figure 4 shows that our method can handle
these situations, by allowing pixels to vote for their instance
centroid with geometry. Methods which rely on watershed
approaches [4], or instance edge identiﬁcation approaches
fail in these scenarios.
To obtain segmentations for each instance, we now need
to estimate the instance centres, ˆin. We propose to con-
sider the estimated instance vectors, ˆxn, as votes in a Hough
parameter space and use a clustering algorithm to identify
these instance centres. OPTICS [2], is an efﬁcient density
based clustering algorithm. It is able to identify an unknown
number of multi-scale clusters with varying density from a
given set of samples. We chose OPICS for two reasons.
Crucially, it does not assume knowledge of the number of
clusters like algorithms such as k-means [33]. Secondly, it
does not assume a canonical instance size or density like
discretised binning approaches [12]. Using OPTICS, we
cluster the points cn + ˆxn into a number of estimated in-
stances, ˆi. We can then assign each pixel, pn to the instance
closest to its estimated instance vector, cn + ˆxn.
Depth Regression. We train with supervised labels us-
ing pixel-wise metric inverse depth using a L1 loss function:
LDepth = 1|ND|
. Our architecture esti-
mates inverse depth, ˆdn, because it can represent points at
inﬁnite distance (such as sky). We can obtain inverse depth
labels, dn, from a RGBD sensor or stereo imagery. Pixels
which do not have an inverse depth label are ignored in the
loss.
5. Experiments
We demonstrate the efﬁcacy of our method on
CityScapes [13], a large dataset for road scene understand-
ing. It comprises of stereo imagery, from automotive grade
stereo cameras with a 22cm baseline, labelled with instance
and semantic segmentations from 20 classes. Depth images
are also provided, labelled using SGM [22], which we treat
as pseudo ground truth. Additionally, we assign zero in-
verse depth to pixels labelled as sky. The dataset was col-
Task Weights
Segmentation
Instance
Inverse Depth
Mean Error [px] Mean Error [px]
Loss
Segmentation only
Instance only
Depth only
Unweighted sum of losses
Approx. optimal weights
2 task uncertainty weighting
2 task uncertainty weighting
2 task uncertainty weighting
3 task uncertainty weighting
Depth
Table 1: Quantitative improvement when learning semantic segmentation, instance segmentation and depth with our multi-task loss.
Experiments were conducted on the Tiny CityScapes dataset (sub-sampled to a resolution of 128 × 256). Results are shown from the
validation set. We observe an improvement in performance when training with our multi-task loss, over both single-task models and
weighted losses. Additionally, we observe an improvement when training on all three tasks (3 × (cid:88)) using our multi-task loss, compared
with all pairs of tasks alone (denoted by 2 × (cid:88)). This shows that our loss function can automatically learn a better performing weighting
between the tasks than the baselines.
lected from a number of cities in ﬁne weather and consists
of 2,975 training and 500 validation images at 2048 × 1024
resolution. 1,525 images are withheld for testing on an on-
line evaluation server.
Further training details, and optimisation hyperparame-
ters, are provided in Appendix A.
5.1. Model Analysis
In Table 1 we compare individual models to multi-task
learning models using a na¨ıve weighted loss or the task un-
certainty weighting we propose in this paper. To reduce the
computational burden, we train each model at a reduced res-
olution of 128 × 256 pixels, over 50, 000 iterations. When
we downsample the data by a factor of four, we also need
to scale the disparity labels accordingly. Table 1 clearly il-
lustrates the beneﬁt of multi-task learning, which obtains
signiﬁcantly better performing results than individual task
models. For example, using our method we improve classi-
ﬁcation results from 59.4% to 63.4%.
We also compare to a number of na¨ıve multi-task losses.
We compare weighting each task equally and using approx-
imately optimal weights. Using a uniform weighting results
in poor performance, in some cases not even improving on
the results from the single task model. Obtaining approxi-
mately optimal weights is difﬁcult with increasing number
of tasks as it requires an expensive grid search over param-
eters. However, even these weights perform worse com-
pared with our proposed method. Figure 2 shows that using
task uncertainty weights can even perform better compared
to optimal weights found through ﬁne-grained grid search.
We believe that this is due to two reasons. First, grid search
is restricted in accuracy by the resolution of the search.
Second, optimising the task weights using a homoscedas-
tic noise term allows for the weights to be dynamic during
training. In general, we observe that the uncertainty term
decreases during training which improves the optimisation
process.
In Appendix B we ﬁnd that our task-uncertainty loss is
robust to the initialisation chosen for the parameters. These
quickly converge to a similar optima in a few hundred train-
ing iterations. We also ﬁnd the resulting task weightings
varies throughout the course of training. For our ﬁnal model
(in Table 2), at the end of training, the losses are weighted
with the ratio 43 : 1 : 0.16 for semantic segmentation, depth
regression and instance segmentation, respectively.
Finally, we benchmark our model using the full-size
CityScapes dataset. In Table 2 we compare to a number of
other state of the art methods in all three tasks. Our method
is the ﬁrst model which completes all three tasks with a sin-
gle model. We compare favourably with other approaches,
outperforming many which use comparable training data
and inference tools. Figure 5 shows some qualitative ex-
amples of our model.
6. Conclusions
We have shown that correctly weighting loss terms is
of paramount importance for multi-task learning problems.
We demonstrated that homoscedastic (task) uncertainty is
an effective way to weight losses. We derived a principled
loss function which can learn a relative weighting automati-
cally from the data and is robust to the weight initialization.
We showed that this can improve performance for scene
understanding tasks with a uniﬁed architecture for seman-
Method
IoU class
Semantic Segmentation
IoU cat
iIoU class
Instance Segmentation
Monocular Disparity Estimation
Multi-Task Learning
Semantic segmentation, instance segmentation and depth regression methods (this work)
78.5
Semantic segmentation and instance segmentation methods
Instance segmentation only methods
Semantic segmentation only methods
-
-
-
DeepLab V3 [10]
PSPNet [44]
Adelaide [31]
Table 2: CityScapes Benchmark [13]. We show results from the test dataset using the full resolution of 1024 × 2048 pixels. For the
full leaderboard, please see www.cityscapes-dataset.com/benchmarks. The disparity (inverse depth) metrics were computed
against the CityScapes depth maps, which are sparse and computed using SGM stereo [21]. Note, these comparisons are not entirely fair,
as many methods use ensembles of different training datasets. Our method is the ﬁrst to address all three tasks with a single model.
(b) Segmentation output
(c) Instance output
(d) Depth output
Figure 5: Qualitative results for multi-task learning of geometry and semantics for road scene understanding. Results are shown
on test images from the CityScapes dataset using our multi-task approach with a single network trained on all tasks. We observe that
multi-task learning improves the smoothness and accuracy for depth perception because it learns a representation that uses cues from other
tasks, such as segmentation (and vice versa).
tic segmentation, instance segmentation and per-pixel depth
regression. We demonstrated modelling task-dependent ho-
moscedastic uncertainty improves the model’s representa-
tion and each task’s performance when compared to sepa-
rate models trained on each task individually.
There are many interesting questions left unanswered.
Firstly, our results show that there is usually not a single op-
timal weighting for all tasks. Therefore, what is the optimal
weighting? Is multitask learning is an ill-posed optimisa-
tion problem without a single higher-level goal?
A second interesting question is where the optimal loca-
tion is for splitting the shared encoder network into separate
decoders for each task? And, what network depth is best for
the shared multi-task representation?
Finally, why do the semantics and depth tasks out-
perform the semantics and instance tasks results in Table 1?
Clearly the three tasks explored in this paper are compli-
mentary and useful for learning a rich representation about
the scene. It would be beneﬁcial to be able to quantify the
relationship between tasks and how useful they would be
for multitask representation learning.
[3] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for scene
IEEE Transactions on Pattern Analysis and
segmentation.
Machine Intelligence, 2017. 1, 5
[7] R. Caruana. Multitask learning. In Learning to learn, pages
[11] R. Collobert and J. Weston. A uniﬁed architecture for natural
language processing: Deep neural networks with multitask
learning. In Proceedings of the 25th international conference
on Machine learning, pages 160–167. ACM, 2008. 1, 2
[14] J. Dai, K. He, and J. Sun. Instance-aware semantic segmenta-
tion via multi-task network cascades. In In Proc. IEEE Conf.
on Computer Vision and Pattern Recognition, 2016. 1
[15] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolu-
tional architecture. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2650–2658, 2015. 1,
2, 3
[17] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
[21] H. Hirschmuller. Accurate and efﬁcient stereo processing by
semi-global matching and mutual information. In In Proc.
IEEE Conf. on Computer Vision and Pattern Recognition,
volume 2, pages 807–814. IEEE, 2005. 8
[22] H. Hirschmuller. Stereo processing by semiglobal matching
and mutual information. IEEE Transactions on pattern anal-
ysis and machine intelligence, 30(2):328–341, 2008. 6
[25] A. Kendall, M. Grimes, and R. Cipolla. Convolutional net-
In Pro-
works for real-time 6-dof camera relocalization.
ceedings of the International Conference on Computer Vi-
sion (ICCV), 2015. 2, 3
[32] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proc. IEEE Conf. on
Computer Vision and Pattern Recognition, 2015. 1
[33] J. MacQueen et al. Some methods for classiﬁcation and anal-
ysis of multivariate observations. In Proceedings of the ﬁfth
Berkeley symposium on mathematical statistics and proba-
bility, volume 1, pages 281–297. Oakland, CA, USA., 1967.
6
[36] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and
transferring mid-level image representations using convolu-
tional neural networks. In In Proc. IEEE Conf. on Computer
Vision and Pattern Recognition, pages 1717–1724. IEEE,
2014. 2
[38] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,
and Y. LeCun. Overfeat: Integrated recognition, localization
International
and detection using convolutional networks.
Conference on Learning Representations (ICLR), 2014. 1, 2,
3
A. Model Architecture Details
We base our model on the recently introduced
DeepLabV3 [10] segmentation architecture. We use
ResNet101 [20] as our base feature encoder, with dilated
convolutions, resulting in a feature map which is downsam-
pled by a factor of 8 compared with the original input im-
age. We then append dilated (atrous) convolutional ASPP
module [10]. This module is designed to improve the con-
textual reasoning of the network. We use an ASPP module
comprised of four parallel convolutional layers, with 256
output channels and dilation rates (1, 12, 24, 36), with ker-
nel sizes (12, 32, 32, 32). Additionally, we also apply global
average pooling to the encoded features, and convolve them
to 256 dimensions with a 1 × 1 kernel. We apply batch
normalisation to each of these layers and concatenate the
resulting 1280 features together. This produces the shared
representation between each task.
We then split the network, to decode this representation
to a given task output. For each task, we construct a decoder
consisting of two layers. First, we apply a 1×1 convolution,
outputting 256 features, followed by batch normalisation
and a non-linear activation. Finally, we convolve this output
to the required dimensions for a given task. For classiﬁca-
tion, this will be equal to the number of semantic classes,
otherwise the output will be 1 or 2 channels for depth or in-
stance segmentation respectively. Finally, we apply bilinear
upsampling to scale the output to the same resolution as the
input.
The majority of the model’s parameters and depth is in
the feature encoding, with very little ﬂexibility in each task
decoder. This illustrates the attraction of multitask learning;
most of the compute can be shared between each task to
learn a better shared representation.
A.1. Optimisation
iter
For all experiments, we use an initial learning rate
of 2.5 × 10−3 and polynomial learning rate decay (1 −
max iter )0.9. We train using stochastic gradient descent,
with Nesterov updates and momentum 0.9 and weight de-
cay 104. We conduct all experiments in this paper using
PyTorch.
For the experiments on the Tiny CityScapes validation
dataset (using a down-sampled resolution of 128 × 256) we
train over 50, 000 iterations, using 256 × 256 crops with
batch size of 8 on a single NVIDIA 1080Ti GPU. We apply
random horizontal ﬂipping to the data.
For the full-scale CityScapes benchmark experiment, we
train over 100, 000 iterations with a batch size of 16. We
apply random horizontal ﬂipping (with probability 0.5) and
random scaling (selected from 0.7 - 2.0) to the data dur-
ing training, before making a 512 × 512 crop. The training
data is sampled uniformly, and is randomly shufﬂed for each
epoch. Training takes ﬁve days on a single computer with
four NVIDIA 1080Ti GPUs.
B. Further Analysis
This task uncertainty loss is also robust to the value we
use to initialise the task uncertainty values. One of the at-
tractive properties of our approach to weighting multi-task
losses is that it is robust to the initialisation choice for the
homoscedastic noise parameters. Figure 6 shows that for an
array of initial choices of log σ2 from −2.0 to 5.0 the ho-
moscedastic noise and task loss is able to converge to the
same minima. Additionally, the homoscedastic noise terms
converges after only 100 iterations, while the network re-
quires 30, 000+ iterations to train. Therefore our model is
robust to the choice of initial value for the weighting terms.
Figure 7 shows losses and uncertainty estimates for each
task during training of the ﬁnal model on the full-size
CityScapes dataset. At a point 500 iterations into training,
the model estimates task variance of 0.60, 62.5 and 13.5 for
semantic segmentation, instance segmentation and depth re-
gression, respectively. Becuase the losses are weighted by
the inverse of the uncertainty estimates, this results in a task
weighting ratio of approximately 23 : 0.22 : 1 between se-
mantics, instance and depth, respectively. At the conclu-
sion of training, the three tasks have uncertainty estimates
of 0.075, 3.25 and 20.4, which results in effective weighting
between the tasks of 43: 0.16 : 1. This shows how the task
uncertainty estimates evolve over time, and the approximate
ﬁnal weightings the network learns. We observe they are far
from uniform, as is often assumed in previous literature.
Interestingly, we observe that this loss allows the net-
work to dynamically tune the weighting. Typically, the ho-
moscedastic noise terms decrease in magnitude as training
progresses. This makes sense, as during training the model
becomes more effective at a task. Therefore the error, and
uncertainty, will decrease. This has a side-effect of increas-
ing the effective learning rate – because the overall uncer-
tainty decreases, the weight for each task’s loss increases.
In our experiments we compensate for this by annealing the
learning rate with a power law.
Finally, a comment on the model’s failure modes. The
model exhibits similar failure modes to state-of-the-art
single-task models. For example, failure with objects out of
the training distribution, occlusion or visually challenging
situations. However, we also observe our multi-task model
tends to fail with similar effect in all three modalities. Ie. an
erroneous pixel’s prediction in one task will often be highly
correlated with error in another modality. Some examples
can be seen in Figure 8.
(a) Semantic segmentation task
(b) Instance segmentation task
(c) Depth regression task
Figure 6: Training plots showing convergence of homoscedastic noise and task loss for an array of initialisation choices for the ho-
moscedastic uncertainty terms for all three tasks. Each plot shows the the homoscedastic noise value optimises to the same solution from
a variety of initialisations. Despite the network taking 10, 000+ iterations for the training loss to converge, the task uncertainty converges
very rapidly after only 100 iterations.
(a) Semantic segmentation task
(b) Instance segmentation task
(c) Depth regression task
Figure 7: Learning task uncertainty. These training plots show the losses and task uncertainty estimates for each task during training.
Results are shown for the ﬁnal model, trained on the fullsize CityScapes dataset.
C. Further Qualitative Results
(b) Semantic segmentation
(c) Instance segmentation
(d) Depth regression
Figure 8: More qualitative results on test images from the CityScapes dataset.
D. Failure Examples
(b) Semantic segmentation
(c) Instance segmentation
(d) Depth regression
Figure 9: Example where our model fails on the CityScapes test data. The ﬁrst two rows show examples of challenging visual effects
such as reﬂection, which confuse the model. Rows three and four show the model incorrectly distinguishing between road and footpath.
This is a common mistake, which we believe is due to a lack of contextual reasoning. Rows ﬁve, six and seven demonstrate incorrect
classiﬁcation of a rare class (bus, fence and motorbike, respectively). Finally, the last two rows show failure due to occlusion and where
the object is too big for the model’s receptive ﬁeld. Additionally, we observe that failures are highly correlated between the modes, which
makes sense as each output is conditioned on the same feature vector. For example, in the second row, the incorrect labelling of the
reﬂection as a person causes the depth estimation to predict human geometry.
