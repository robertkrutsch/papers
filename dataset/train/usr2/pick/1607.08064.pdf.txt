CNN-based Patch Matching for Optical Flow with Thresholded Hinge
Embedding Loss
Christian Bailer1
Kiran Varanasi1
Didier Stricker1,2
Christian.Bailer@dfki.de
Didier.Stricker@dfki.de
1German Research Center for Artiﬁcial Intelligence (DFKI), 2University of Kaiserslautern
Kiran.Varanasi@dfki.de
Abstract
Learning based approaches have not yet achieved their
full potential in optical ﬂow estimation, where their perfor-
mance still trails heuristic approaches.
In this paper, we
present a CNN based patch matching approach for opti-
cal ﬂow estimation. An important contribution of our ap-
proach is a novel thresholded loss for Siamese networks. We
demonstrate that our loss performs clearly better than ex-
isting losses. It also allows to speed up training by a factor
of 2 in our tests. Furthermore, we present a novel way for
calculating CNN based features for different image scales,
which performs better than existing methods. We also dis-
cuss new ways of evaluating the robustness of trained fea-
tures for the application of patch matching for optical ﬂow.
An interesting discovery in our paper is that low-pass ﬁlter-
ing of feature maps can increase the robustness of features
created by CNNs. We proved the competitive performance
of our approach by submitting it to the KITTI 2012, KITTI
2015 and MPI-Sintel evaluation portals where we obtained
state-of-the-art results on all three datasets.
1. Introduction
In recent years, variants of the PatchMatch [5] approach
showed not only to be useful for nearest neighbor ﬁeld
estimation, but also for the more challenging problem of
large displacement optical ﬂow estimation. So far, most
top performing methods like Deep Matching [32] or Flow
Fields [3] strongly rely on robust multi-scale matching
strategies, while they still use engineered features (data
terms) like SIFTFlow [22] for the actual matching.
On the other hand, works like [30, 34] demonstrated
the effectiveness of features based on Convolutional Neu-
ral Network (CNNs) for matching patches. However, these
works did not validate the performance of their features us-
ing an actual patch matching approach like PatchMatch or
Flow Fields that matches all pixels between image pairs. In-
stead, they simply treat matching patches as a classiﬁcation
problem between a predeﬁned set of patches.
This ignores many practical issues. For instance, it is
important that CNN based features are not only able to dis-
tinguish between different patch positions, but the position
should also be determined accurately. Furthermore, the top
performing CNN architectures are very slow when used for
patch matching as it requires matching several patches for
every pixel in the reference image. While Siamese networks
with L2 distance [30] are reasonably fast at testing time
and still outperform engineered features regarding classiﬁ-
cation, we found that they are usually underperforming en-
gineered features regarding (multi-scale) patch matching.
We think that this has among other things (see Section 4)
to do with the convolutional structure of CNNs: as neigh-
boring patches share intermediate layer outputs it is much
easier for CNNs to learn matches of neighboring patches
than non neighboring patches. However, due to propaga-
tion [5] correctly matched patches close to each other usu-
ally contribute less for patch matching than patches far apart
from each other. Classiﬁcation does not differentiate here.
A ﬁrst solution to succeed in CNN based patch match-
ing is to use pixel-wise batch normalization [12]. While it
weakens the unwanted convolutional structure, it is compu-
tationally expensive at test time. Thus, we do not use it.
Instead, we improve the CNN features themselves to a level
that allows us to outperform existing approaches.
Our ﬁrst contribution is a novel loss function for the
Siamese architecture with L2 distance [30]. We show that
the hinge embedding loss [30] which is commonly used for
Siamese architectures and variants of it have an important
design ﬂaw: they try to decrease the L2 distance unlimit-
edly for correct matches, although very small distances for
patches that differ due to effects like illumination changes or
partial occlusion are not only very costly but also unneces-
sary, as long as false matches have larger L2 distances. We
demonstrate that we can signiﬁcantly increase the matching
quality by relaxing this ﬂaw.
Furthermore, we present a novel way to calculate CNN
based features for the scales of Flow Fields [3], which
clearly outperforms the original multi-scale feature creation
approach, with respect to CNN based features. Doing so,
an important ﬁnding is that low-pass ﬁltering CNN based
feature maps robustly improves the matching quality.
Moreover, we introduce a novel matching robustness
measure that is tailored for binary decision problems like
patch matching (while ROC and PR are tailored for classi-
ﬁcation problems). By plotting the measure over different
displacements and distances between a wrong patch and the
correct one we can reveal interesting properties of different
loss functions and scales. Our main contributions are:
1. A novel loss function, that clearly outperforms other
state-of-the art losses in our tests and allows to speed
up training by a factor of around two.
2. A novel multi-scale feature creation approach tailored
for CNN features for optical ﬂow.
3. New evaluation measure of matching robustness for
optical ﬂow and corresponding plots.
4. We show that low-pass ﬁltering the feature maps cre-
ated by CNNs improves matching robustness.
5. We demonstrate the effectiveness of our approach by
obtaining a top performance on all three major eval-
uation portals KITTI 2012 [14], 2015 [25] and MPI-
Sintel [8]. Former learning based approaches always
trailed heuristic approaches on at least one of them.
While regularized optical ﬂow estimation goes back to
Horn and Schunck [18], randomized patch matching [5] is
a relatively new ﬁeld, ﬁrst successfully applied in approx-
imate nearest neighbor estimation where the data term is
well-deﬁned. The success in optical ﬂow estimation (where
the data term is not well-deﬁned) started with publications
like [4, 10]. One of the most recent works is Flow Fields [3],
which showed that with proper multi-scale patch matching,
top performing optical ﬂow results can be achieved.
Regarding patch or descriptor matching with learned
data terms, there exists a fair amount of literature [17, 30,
34, 31]. These approaches treat matching at an abstract
level and do not present a pipeline to solve a problem
like optical ﬂow estimation or 3D reconstruction, although
many of them use 3D reconstruction datasets for evaluation.
Zagoruyko and Komodakis [34] compared different archi-
tectures to compare patches. Simo-Serra et al. [30] used the
Siamese architecture [6] with L2 distance. They argued that
it is the most useful one for practical applications.
Recently, several successful CNN based approaches for
stereo matching appeared [35, 23, 24]. However, so far
there are still few approaches that successfully use learning
to compute optical ﬂow. Worth mentioning is FlowNet [11].
They tried to solve the optical ﬂow problem as a whole with
CNNs, having the images as CNN input and the optical ﬂow
Layer
Type
Input size
Kernel size
Out. channels
Stride
Nonlinearity Tanh
Tanh Tanh Tanh
Tanh Tanh
Table 1. The CNN architecture used in our experiments.
as output. While the results are good regarding runtime,
they are still not state-of-the-art quality. Also, the network
is tailored for a speciﬁc image resolution and to our knowl-
edge training for large images of several megapixel is still
beyond todays computational capacity.
A ﬁrst approach using patch matching with CNN based
features is PatchBatch [12]. They managed to obtain state-
of-the-art results on the KITTI dataset [14], due to pixel-
wise batch normalization and a loss that includes batch
statistics. However, pixel-wise batch normalization is com-
putationally expensive at test time. Furthermore, even with
pixel-wise normalization their approach trails heuristic ap-
proaches on MPI-Sintel [8]. A recent approach is Deep-
DiscreteFlow [15] which uses DiscreteFlow [26] as basis
instead of patch matching. Despite using recently invented
dilated convolutions [23] (we do not use them, yet) they also
trail the original DiscreteFlow approach on some datasets.
Our approach is based on a Siamese architecture [6]. The
aim of Siamese networks is to learn to calculate a mean-
ingful feature vector D(p) for each image patch p. During
training the L2 distance between feature vectors of match-
ing patches (p1 ≡ p+
2 ) is reduced, while the L2 distance be-
tween feature vectors of non-matching patches (p1 (cid:54)= p−
2 )
is increased (see [30] for a more detailed description).
Siamese architectures can be strongly speed up at test-
ing time as neighboring patches in the image share convo-
lutions. Details on how the speedup works are described
in our supplementary material. The network that we used
for our experiments is shown in Table 1. Similar to [7], we
use Tanh nonlinearity layers as we also have found them to
outperform ReLU for Siamese based patch feature creation.
3.1. Loss Function and Batch Selection
The most common loss function for Siamese network
(2)
It tries to minimize the L2 distance of matching patches
and to increase the L2 distance of non-matching patches
Figure 1. If a sample is pushed (blue arrow), although it is clearly
on the correct side of the decision boundary other samples also
move due to weight change. If most samples are classiﬁed cor-
rectly beforehand, this creates more false decision boundary cross-
ings than correct ones. lh performs the unnecessary push, lt not.
above m. An architectural ﬂaw which is not or only poorly
treated by existing loss functions is the fact that the loss
pushes feature distances between matching patches unlimit-
2 ) → 0). We think that training up to
edly to zero (L2(p1, p+
very small L2 distances for patches that differ due to effects
like rotation or motion blur is very costly – it has to come
at the cost of failure for other pairs of patches. A possible
explanation for this cost is shown in Figure 1. As a result,
we introduce a modiﬁed hinge embedding loss with thresh-
old t that stops the network from minimizing L2 distances
too much:
(3)
We add t also to the second equation to keep the “virtual
decision boundary” at m/2. This is not necessary but makes
comparison between different t values fairer.
As our goal is a network that creates features with the
property L2(p1, p+
2 ) one might argue that it
is better to train this property directly. A known function to
do this is a gap based loss [17, 33], that only keeps a gap in
the L2 distance between matching and non-matching pairs:
lg(p1, p−
2 ) (reverse gradient). While lg
intuitively seems to be better suited for the given problem
than lt, we will show in Section 4 why this is not the case.
There we will also compare lt to further loss functions.
The given loss functions have in common that the loss
gradient is sometimes zero. Ordinary approaches still back
propagate a zero gradient. This not only makes the approach
slower than necessary, but also leads to a variable effective
batch size of training samples, that are actually back propa-
gated. This is a limited issue for the hinge embedding loss
lh, where only ≈ 25% of the training samples obtain a zero
gradient in our tests. However, with lt (and suitable t) more
than 80% of the samples obtain a zero gradient.
As a result, we only add training samples with a non-zero
loss to a batch. All other samples are rejected without back
propagation. This not only increases the training speed by
a factor of around two in our tests, but also improves the
training quality by avoiding variable effective batch sizes.
3.2. Training
Our training set consists of several pairs of images (I1,
I2 ∈ Iall) with known optical ﬂow displacement between
their pixels. We ﬁrst subtract the mean from each image
and divide it by its standard derivation. To create training
samples, we randomly extract patches p1 ∈ I1 and their
corresponding matching patches p+
2 for
positive training samples. For each p1, we also extract one
2 ∈ I2, p1 (cid:54)= p−
non-matching patch p−
2 for negative training
samples. Negative samples p−
2 are sampled from a distribu-
tion N (p+
2 ) that prefers patches close to the matching patch
p+
2 , with a minimum distance to it of 2 pixels, but it also
allows to sample patches that are far from p+
2 . The exact
distribution can be found in our supplementary material.
We only train with pairs of patches where the center
pixel of p1 is not occluded in the matching patch p+
2 . Oth-
erwise, the network would train the occluding object as a
positive match. However, if the patch center is visible we
expect the network to be able to deal with a partial occlu-
sion. We use a learning rate between 0.004 and 0.0004 that
decreases linearly in exponential space after each batch i.e.
learnRate(t) = e−xt → learnRate(t + 1) = e−(xt+).
3.3. Multi-scale matching
The Flow Fields approach [3], which we use as basis
for our optical ﬂow pipeline compares patches at different
scales using scale spaces [21], i.e. all scales have the full
image resolution. It creates feature maps for different scales
by low-pass ﬁltering the feature map of the highest scale
(Figure 2 left). For SIFTFlow [22] features used in [3],
low-pass ﬁltering features (i.e. feature → low-pass = fea-
ture → downsample → upsample) performs better than re-
calculating features for each scale on a different resolution
(i.e. downsample → feature → upsample).
We observed the same effect for CNN based features –
even if the CNN is also trained on the lower resolutions.
However, with our modiﬁcations shown in Figure 2 right
(that are further motivated in Section 4), it is possible to
obtain better results by recalculating features on different
resolutions. We use a CNN trained and applied only on the
highest image resolution for the highest and second highest
scale. Furthermore, we use a CNN trained on 3 resolutions
(100%, 50% and 25%) to calculate the feature maps for the
third and fourth scale applied at 50% and 25% resolution,
respectively. For the multi-resolution CNN, the probability
to select a patch on a lower resolution for training is set to
be 60% of the probability for the respective next higher res-
olution. For lower resolutions, we also use the distribution
N (p+
2 ). This leads to a more wide spread distribution with
Figure 2. Our modiﬁcation of feature creation of the Flow Fields approach [3] for clearly better CNN performance. Note that Flow Fields
expects feature maps of all scales in the full image resolution (See [3] for details). Reasons of design decision can be found in Section 4.1.
respect to the full image resolution.
Feature maps created by our CNNs are not used directly.
Instead, we perform a 2x low-pass ﬁlter on them, before
using them. Low-pass ﬁltering image data creates matching
invariance while increasing ambiguity (by removing high
frequency information). Assuming that CNNs are unable to
create perfect matching invariance, we can expect a similar
effect on feature maps created by CNNs. In fact, a small
low-pass ﬁlter clearly increases the matching robustness.
The Flow Fields approach [3] uses a secondary con-
sistency check with different patch size. With our ap-
proach, this would require to train and execute two addi-
tional CNNs. To keep it simple, we perform the secondary
check with the same features. This is possible due to the
fact that Flow Fields is a randomized approach. Still, our
tests with the original features show that a real secondary
consistency check performs better. The reasoning for our
design decisions in Figure 2 can be found in Section 4.1.
3.4. Evaluation Methodology for Patch Matching
In previous works, the evaluation of the matching robust-
ness of (learning based) features was performed by evalua-
tion methods commonly used in classiﬁcation problems like
ROC in [7, 34] or PR in [30]. However, patch matching is
not a classiﬁcation problem, but a binary decision problem.
While one can freely label data in classiﬁcation problems,
patch matching requires to choose, at each iteration, out of
two proposal patches p2, p∗
2 the one that ﬁts better to p1.
The only exception from this rule is outlier ﬁltering. This is
not really an issue, as there are better approaches for outlier
ﬁltering, like the forward backward consistency check [3],
which is more robust than matching-error based outlier ﬁl-
tering1. In our evaluation, the matching robustness r of a
network is determined as the probability that a wrong patch
p−
2 is not confused with the correct patch p+
2 :
1Even if outlier ﬁltering would be performed by matching error, the
actual matching remains a decision problem.
1. The curve for different spatial distances between p+
2
2. The curve for different optical ﬂow displacements be-
rdist and rf low vary strongly for different locations. This
makes differences between different networks hard to visu-
alize. For better visualization, we plot the relative matching
robustness errors Edist and Ef low, computed with respect
to a pre-selected network net1. E is deﬁned as:
4. Evaluation
We examine our approach on the KITTI 2012 training
set [14] as it is one of the few datasets that contains ground
truth for non-synthetic large displacement optical ﬂow esti-
mation. We use patches taken from 130 of the 194 images
of the set for training and patches from the remaining 64
images for validation. Each tested network is trained with
10 million negative and 10 million positive samples in total.
Furthermore, we publicly validate the performance of our
approach by submitting our results to the KITTI 2012, the
recently published KITTI 2015 [25] and MPI-Sintel evalu-
ation portals (with networks trained on the respective train-
ing set). We use the original parameters of the Flow Fields
approach [3] except for the outlier ﬁlter distance  and the
random search distance R.  is set to the best value for each
network (with accuracy ±0.25, mostly:  = 1.5). The ran-
dom search distance R is set to 2 for four iterations and to
R = 1 for two additional iterations to increase accuracy.
The batch size is set to 100 and m to 1.
To evaluate the quality of our optical ﬂow results we
calculate the endpoint error (EPE) for non-occluded areas
Approach
ours
EPE all
Approach
all resolutions
EPE all
Table 2. Comparison of CNN based multi-scale feature creation approaches. See text for details.
Figure 3. Relative matching robustness errors Edist(“No Downsampling”, X). Features created on lower resolutions are more accurate for
large distances but less accurate for small ones. No downsampling is on the horizontal line as results are normalized for it. Details in text.
(noc) as well as occluded + non-occluded areas (all). (noc)
is a more direct measure as CNNs are only trained here.
However, the interpolation into occluded areas (like Flow
Fields we use EpicFlow [28] for that) also depends on good
matches close to the occlusion boundary, where matching
is especially difﬁcult due to partial occlusions of patches.
Furthermore, like [14], we measure the percentage of pixels
with an EPE above a threshold in pixels (px).
4.1. Comparison of CNN based Multi-Scale Feature
Map Approaches
In Table 2, we compare the original feature creation ap-
proach (Figure 2 left) with our approach (Figure 2 right),
with respect to our CNN features. We also examine two
variants of our approach in the table: nolowpass which
does not contain the “Low-Pass 2x” blocks and all resolu-
tions which uses 1x,2x,4x,8x up/downsampling for the four
scales (instead of 1x,1x,2x,4x in Figure 2 right). The reason
why all resolutions does not work well is demonstrated in
Figure 3 (a). Starting from a distance between p+
2 of
9 pixels, CNN based features created on a 2x down-sampled
image match more robustly than CNN based features cre-
ated on the full image resolution. This is insufﬁcient as the
random search distance on scale 2 is only 2R = 4 pixels.
Thus, we use it for scale 3 (with random search distance
4R = 8 ≈ 9 pixels).
One can argue that by training the CNN with more close-
2 ) more accuracy could be gained. But
by samples Nclose(p+
Approach/
Loss
Lh
EPE all
raising extremely the amount of close-by samples only re-
duces the accuracy threshold from 9 to 8 pixels. Using a
CNN with smaller 32x32 patches instead of 56x56 patches
does not raise the accuracy either– it even clearly decreases
it. Figure 3 (b) shows that downsampling decreases the
matching robustness error signiﬁcantly for larger distances.
In fact, for a distance above 170 pixels, the relative error of
4x downsampling is reduced by nearly 100% compared to
No downsampling – which is remarkable.
Multi-resolution network training We examine three
variants of training our multi-resolution network (green
boxes in Figure 2): training it on 100%, 50% and 25% res-
olution although it is only used for 50% and 25% resolu-
tion, at testing time (ours in Table 2), training it on 50%
and 25% resolutions, where it is used for at testing time (ms
res 2+) and training it only on 100% resolution (ms res 1).
As can be seen in Table 2 training on all resolutions (ours)
clearly performs best. Likely, mixed training data performs
best as samples of the highest resolution provide the largest
entropy while samples of lower resolutions ﬁt better to the
problem. However, training samples of lower resolutions
seem to harm training for higher resolutions. Therefore, we
use an extra CNN for the highest resolution.
4.2. Loss Functions and Mining
We compare our loss lt to other state-of-the-art losses
and Hard Mining [30] in Figure 5 and Table 3. As shown in
the table, our thresholded loss lt with t = 0.3 clearly outper-
forms all other losses. DrLIM [16] reduces the mentioned
ﬂaw in the hinge loss, by training samples with small hinge
loss less. While this clearly reduces the error compared to
hinge, it cannot compete with our thresholded loss lt. Fur-
thermore, no speedup during training is possible like with
our approach. CENT. (CENTRIFUGE) [12] is a variant of
DrLIM which performs worse than DrLIM in our tests.
Hard Mining [30] only trains the hardest samples with
the largest hinge loss and thus also speeds up training. How-
ever, the percentage of samples trained in each batch is ﬁxed
and does not adapt to the requirements of the training data
like in our approach. With our data, Hard Mining becomes
unstable with a mining factor above 2 i.e. the loss of nega-
tive samples becomes much larger than the loss of positive
samples. This leads to poor performance (r = 96.61% for
Hard Mining x4). We think this has to do with the fact that
the hardest of our negative samples are much harder to train
than the hardest positive samples. Some patches are e.g.
fully white due to overexposure (negative training has no
effect here). Also, many of our negative samples have, in
contrast to the samples of [30], a very small spatial distance
to their positive counterpart. This makes their training even
harder (We report most failures for small distances, see sup-
plementary material), while positive samples do not change.
To make sure that our dynamic loss based mining ap-
proach (Lt with t = 0.3) cannot become unstable towards
much larger negative loss values we tested it to an extreme:
we randomly removed 80% of the negative training sam-
ples while keeping all positive. Doing so, it not only stayed
stable, but it even used a smaller positive/negative sample
mining ratio than the approach with all training samples –
possibly it can choose harder positive samples which con-
tribute more to training. Even with the removal of 80% (8
million) of possible samples we achieved a matching ro-
Figure 4. The distribution of L2 errors for different for Lt and Lg
−
for positive samples p+
2 with distance of
2 and negative samples p
10 pixels to the corresponding positive sample.
bustness r of 99.18%.
Lg performed best for g = 0.4 which corresponds to a
gap of Lt, t = 0.3 (gLt = 1 − 2t). However, even with
the best g, Lg performs signiﬁcantly worse than Lt. This is
probably due to the fact that the variance Var(L2(p1, p2)) is
much larger for Lg than for Lt. As shown in Figure 4, this is
the case for both positive (p+
2 ) sam-
ples. We think this affects the test set negatively as follows:
2 , p−
if we assume that p1, p+
2 are unlearned test set patches it
is clear that the condition L2(p1, p+
2 ) is more
likely violated if Var(L2(p1, p+
2 )) are
large compared to the learned gap. Only with Lt it is pos-
sible to force the network to keep the variance small com-
pared to the gap. With Lg it is only possible to control the
gap but not the variance, while lh keeps the variance small
but cannot limit the gap.
Matching Robustness plots Some loss functions perform
worse than others although they have a larger matching ro-
bustness r. This mostly can be explained by the fact that
they perform poorly for large displacements (as shown in
Figure 5 (b)). Here, correct matches are usually more im-
portant as missing matches lead lo larger endpoint errors.
An averaged r over all pixels does not consider this.
Figure 5 also shows the effect of parameter t in Lt. Up to
t ≈ 0.3, all distances and ﬂow displacements are improved,
while small distances and displacements beneﬁt more and
up to a larger t ≈ 0.4. The improvement happens as un-
necessary destructive training is avoided (see Section 3.1).
Patches with small distances beneﬁt more form larger t,
likely as the real gap greal = |L2(p1, p−
2 ) − L2(p1, p+
2 )|
is smaller here (as p−
2 are very similar for small dis-
tances). For large displacements patches get more chaotic
(due to more motion blur, occlusions etc.), which forces
larger variances of the L2 distances and thus a larger gap
is required to counter the larger variance.
Lg performs worse than Lt mainly at small distances and
large displacements. Likely, the larger variance is more de-
structive for small distances, as the real gap greal is smaller
Figure 5. Relative matching robustness errors E(“Lt, t = 0.3(cid:48)(cid:48), X) for different loss functions plotted for different distances (a) and
displacements (b). Note that the plot for Lt, t = 0.3 is on the horizontal line, as E is normalized for it. See text for details.
(a) by distance between p+
(more sensitive) here. Figure 5 also shows that low-pass ﬁl-
tering the feature map increases the matching robustness for
all distances and displacements. In our tests, a 2.25× low-
pass performed the best (tested with ±0.25). Engineered
SIFTFlow features can beneﬁt from much larger low-pass
ﬁlters which makes the original pipeline (Figure 2 left) ex-
tremely efﬁcient for them. However, using them with our
pipeline (which recalculates features on different resolu-
tions) shows that their low matching robustness is justiﬁed
(see Table 3). SIFTFlow also performs better in outlier ﬁl-
tering. Due to such effects that can so far not directly be
trained, it is still challenging to beat well designed purely
heuristic approaches with learning. In fact, existing CNN
based approaches often still underperform purely heuristic
approaches – even direct predecessors (see Section 4.3).
Our public results on the KITTI 2012 [14], 2015 [25] and
MPI-Sintel [8] evaluation portals are shown in Table 4, 5
and 6. For the public results we used 4 extra iterations with
R = 1 for best possible subpixel accuracy and for simi-
lar runtime to Flow Fields [3]. t is set to 0.3. On KITTI
2012 our approach is the best in all measures, although
we use a smaller patch size than PatchBatch (71x71) [12].
PatchBatch (51x51) with a patch size more similar to ours
performs even worse. PatchBatch*(51x51) which is like
our work without pixel-wise batch normalization even trails
purely heuristic methods like Flow Fields.
On KITTI 2015 our approach also clearly outperforms
PatchBatch and all other general optical ﬂow methods in-
cluding DeepDiscreteFlow [15] that, despite using CNNs,
trails its engineered predecessor DiscreteFlow [26] in many
measures. The only methods that outperform our approach
are the rigid segmentation based methods SDF [1], JFS [20]
and SOF [29]. These require segmentable rigid objects
moving in front of rigid background and are thus not suited
for scenes that contain non-rigid objects (like MPI-Sintel)
or objects which are not easily segmentable. Despite not
making any such assumptions our approach outperforms
two of them in the challenging foreground (moving cars
with reﬂections, deformations etc.). Furthermore, our ap-
proach is clearly the fastest of all top performing methods
although there is still optimization potential (see below).
Especially, the segmentation based methods are very slow.
On the non rigid MPI-Sintel datasets our approach is
the best in the non-occluded areas, which can be matched
by our features.
Interpolation into occluded areas with
EpicFlow [28] works less well, which is no surprise as as-
pects like good outlier ﬁltering which are important for oc-
cluded areas are not learned by our approach. Still, we ob-
tained the best overall result on the more challenging ﬁnal
set that contains motion blur. In contrast, PatchBatch lags
far behind on MPI-Sintel, while DeepDiscreteFlow again
clearly trails its predecessor DiscreteFlow on the clean set,
but not the ﬁnal set. Our approach never trails on the rele-
vant matchable (non-occluded) part.
Our detailed runtime is 4.5s for CNNs (GPU) + 16.5s
patch matching (CPU) + 2s for up/downsampling and low-
pass (CPU). The CPU parts of our approach likely can be
signiﬁcantly sped up using GPU versions like a GPU based
propagation scheme [2, 13] for patch matching. This is
contrary to PatchBatch where the GPU based CNN already
takes the majority of time (due to pixel-wise normalization).
Also, in ﬁnal tests (after submitting to evaluation portals)
we were able to improve our CNN architecture (see sup-
plementary material) so that it only needs 2.5s with only a
marginal change in quality on our validation set.
Method
runtime
Table 4. Results on KITTI 2012 [14] test set. Numbers in brackets show the patch size for learning based methods. Best result for published
methods is bold, 2. best is underlined. PatchBatch* is PatchBatch without pixel-wise batch normalization.
background
foreground (cars)
total
Type
Rigid
Segmentation
based methods
General
methods
Method
DeepDiscreteFlow [15]
runtime
Table 5. Results on KITTI 2015 [25] test set. Numbers in brackets shows the used patch size for learning based methods. Best result for all
published general optical ﬂow methods is bold, 2. best underlined. Bold for segmentation based method shows that the result is better than
the best general method. Rigid segmentation based methods were designed for urban street scenes and similar containing only segmentable
rigid objects and rigid background (and are usually very slow), while general methods work for all optical ﬂow problems.
5. Conclusion and Future Work
In this paper, we presented a novel extension to the hinge
embedding loss that not only outperforms other losses in
learning robust patch representations, but also allows to in-
crease the training speed and to be robust with respect to
unbalanced training data. We presented a new multi-scale
feature creation approach for CNNs and proposed new eval-
uation measures by plotting matching robustness with re-
spect to patch distance and motion displacement. Further-
more, we showed that low-pass ﬁltering feature maps cre-
ated by CNNs improves the matching result. All together,
we proved the effectiveness of our approach by submitting
it to the KITTI 2012, KITTI 2015 and MPI-Sintel evalua-
tion portals where we, as the ﬁrst learning based approach,
achieved state-of-the-art results on all three datasets. Our
results also show the transferability of our contribution, as
our ﬁndings made in Section 4.1 and 4.2 (on which our
architecture is based on) are solely based on KITTI 2012
validation set, but still work unchanged on KITTI 2015 and
MPI-Sintel test sets, as well.
In future work, we want to improve our network archi-
tecture (Table 1) by using techniques like (non pixel-wise)
batch normalization and dilated convolutions [23]. Further-
more, we want to ﬁnd out if low-pass ﬁltering invariance
also helps in other application, like sliding window object
detection [27]. We want to further improve our loss func-
tion Lt e.g. by a dynamic t that depends on the properties
of training samples. So far, we just tested a patch size of
56x56 pixels, although [12] showed that larger patch sizes
Method(ﬁnal)
Ours
DeepDiscreteFlow[15]
Ours
DeepDiscreteFlow[15]
Table 6. Results on MPI-Sintel [8]. Best result for all published
methods is bold, second best is underlined.
can perform even better. It might be interesting to ﬁnd out
which is the largest beneﬁcial patch size. Frames of MPI-
Sintel with very large optical ﬂow showed to be especially
challenging. They lack training data due to rarity, but still
have a large impact on the average EPE (due to huge EPE).
We want to create training data tailored for such frames and
examine if learning based approaches beneﬁt from it.
Acknowledgments
This work was funded by the BMBF project DYNAM-
References
[1] M. Bai, W. Luo, K. Kundu, and R. Urtasun. Exploiting se-
mantic information and deep matching for optical ﬂow. In
European Conference on Computer Vision (ECCV), 2016. 7,
8
[2] C. Bailer, M. Finckh, and H. P. Lensch. Scale robust multi
In European Conference on Computer Vision
[4] L. Bao, Q. Yang, and H. Jin. Fast edge-preserving patch-
match for large displacement optical ﬂow. In Computer Vi-
sion and Pattern Recognition (CVPR), 2014. 2
[5] C. Barnes, E. Shechtman, A. Finkelstein, and D. Goldman.
Patchmatch: A randomized correspondence algorithm for
structural image editing. ACM Transactions on Graphics-
TOG, 2009. 1, 2
[8] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
naturalistic open source movie for optical ﬂow evaluation.
In European Conference on Computer Vision (ECCV), 2012.
http://sintel.is.tue.mpg.de/results. 2, 7, 8
[9] Q. Chen and V. Koltun. Full ﬂow: Optical ﬂow estimation by
global optimization over regular grids. In Computer Vision
and Pattern Recognition (CVPR), 2016. 8
[13] S. Galliani, K. Lasinger, and K. Schindler. Massively par-
allel multiview stereopsis by surface normal diffusion.
In
International Conference on Computer Vision (ICCV), 2015.
7
robotics:
Journal
The kitti dataset.
sion meets
ternational
http://www.cvlibs.net/datasets/kitti/
eval_stereo_flow.php?benchmark=flow. 2, 4,
5, 7, 8
of Robotics Research,
[16] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduc-
tion by learning an invariant mapping. In Computer Vision
and Pattern Recognition (CVPR), 2006. 5, 6
match for large displacement optical ﬂow. 8
[20] J. Hur and S. Roth. Joint optical ﬂow and temporally con-
sistent semantic segmentation. In European Conference on
Computer Vision (ECCV), 2016. 7, 8
[24] N. Mayer, E. Ilg, P. H¨ausser, P. Fischer, D. Cremers,
A. Dosovitskiy, and T. Brox. A large dataset to train con-
volutional networks for disparity, optical ﬂow, and scene
ﬂow estimation. In Computer Vision and Pattern Recogni-
tion (CVPR), 2016. 2
vehicles.
autonomous
Object
scene ﬂow
In Computer Vision
http:
[27] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
Neural Information Processing Systems (NIPS), 2015. 8
[28] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
Epicﬂow: Edge-preserving interpolation of correspondences
for optical ﬂow. In Computer Vision and Pattern Recognition
(CVPR), 2015. 5, 7
[32] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid.
Deepﬂow: Large displacement optical ﬂow with deep match-
In International Conference on Computer Vision
ing.
(ICCV), 2013. 1
[33] P. Wohlhart and V. Lepetit. Learning descriptors for object
recognition and 3d pose estimation. In Computer Vision and
Pattern Recognition (CVPR), 2015. 3
[34] S. Zagoruyko and N. Komodakis. Learning to compare im-
age patches via convolutional neural networks. In Computer
Vision and Pattern Recognition (CVPR), 2015. 1, 2, 4
